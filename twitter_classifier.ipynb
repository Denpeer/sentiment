{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main notebook file\n",
    "This file contains most of the work done on the twitter classifier and textblob comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.stem\n",
    "import nltk\n",
    "from sklearn import cross_validation\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet\n",
    "import sys, os\n",
    "\n",
    "def blockPrint():\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "def enablePrint():\n",
    "    sys.stdout = sys.__stdout__\n",
    "\n",
    "from sematch.semantic.similarity import WordNetSimilarity\n",
    "wns = WordNetSimilarity()\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import sts gold dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id  polarity                                              tweet\n0  1467933112         0  the angel is going to miss the athlete this we...\n1  2323395086         0  It looks as though Shaq is getting traded to C...\n2  1467968979         0     @clarianne APRIL 9TH ISN'T COMING SOON ENOUGH \n3  1990283756         0  drinking a McDonalds coffee and not understand...\n4  1988884918         0  So dissapointed Taylor Swift doesnt have a Twi...\n"
     ]
    }
   ],
   "source": [
    "sts_gold = pd.read_csv('sts_gold_v03/sts_gold_tweet.csv', sep=';')\n",
    "print(sts_gold.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing\n",
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_tokenize(input_dataset):\n",
    "    sts_gold_tokenized = input_dataset.copy()\n",
    "    \n",
    "    # sts_gold_tokenized.drop('tweet',axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "    for i,row in input_dataset.iterrows():\n",
    "        sts_gold_tokenized.at[i,'tweet'] = wordpunct_tokenize(row['tweet'].lower())\n",
    "\n",
    "        \n",
    "    # print(sts_gold_tokenized.head())\n",
    "    return sts_gold_tokenized\n",
    "\n",
    "\n",
    "sts_gold_tokenized = preprocess_tokenize(sts_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks as though Shaq is getting traded to Cleveland to play w/ LeBron... Too bad for Suns' fans. The Big Cactus is no more \n['It', 'looks', 'as', 'though', 'Shaq', 'is', 'getting', 'traded', 'to', 'Cleveland', 'to', 'play', 'w', '/', 'LeBron', '...', 'Too', 'bad', 'for', 'Suns', \"'\", 'fans', '.', 'The', 'Big', 'Cactus', 'is', 'no', 'more']\n"
     ]
    }
   ],
   "source": [
    "print(sts_gold['tweet'][1])\n",
    "print(wordpunct_tokenize(sts_gold['tweet'][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_stemming(input_dataset):\n",
    "    sts_gold_stemmed = input_dataset.copy()\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    # print(ps.stem('getting'))\n",
    "    \n",
    "    \n",
    "    # sts_gold_tokenized.drop('tweet',axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "    for i,row in input_dataset.iterrows():\n",
    "        sts_gold_stemmed.at[i,'tweet'] = [ps.stem(word) for word in row['tweet']]\n",
    "        \n",
    "    # print(sts_gold_stemmed.head())\n",
    "    return sts_gold_stemmed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS-Tagging (filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pos(input_dataset):\n",
    "    \n",
    "    def check_adj(word):\n",
    "        synsets = wordnet.synsets(word)\n",
    "        for ss in synsets:\n",
    "            if ss.pos() in ['a'] :\n",
    "                return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    sts_gold_pos = input_dataset.copy()\n",
    "    \n",
    "    # print(input_dataset.at[0,'tweet'])\n",
    "    # print(nltk.pos_tag(input_dataset.at[0,'tweet']))\n",
    "    # \n",
    "    # sts_gold_tokenized.drop('tweet',axis=1,inplace=True)\n",
    "    \n",
    "    # accepted_tags= ['JJ','JJR','JJS','RB','RBR','RBS','VB','VBD','VBG','VBN','VBP','VBZ','MD','NN','NNS','NNP','NNPS']\n",
    "\n",
    "    accepted_tags=['JJ','JJR','JJS','RB','RBR','RBS','VB','VBD','VBG','VBN','VBP','VBZ','VBG']\n",
    "    # \n",
    "    \n",
    "    # accepted_tags=['JJ','JJR','JJS','RB','RBR','RBS']\n",
    "    \n",
    "    # print(nltk.pos_tag(wordpunct_tokenize('@JBsFanArgentina Hey I luv this pic!!! was amazing of the last CHAT of The JB in FACEBOOK!'.lower())))\n",
    "\n",
    "    \n",
    "    pos_tags_found = []\n",
    "    empty_lists = 0\n",
    "    for i,row in input_dataset.iterrows():\n",
    "        # pos = [word[0] for word in nltk.pos_tag(row['tweet']) if check_adj(word[0])]\n",
    "        pos = [word[0] for word in nltk.pos_tag(row['tweet']) if word[1] in accepted_tags or check_adj(word[0])]\n",
    "        if len(pos) < 1:\n",
    "            empty_lists += 1\n",
    "        \n",
    "        sts_gold_pos.at[i,'tweet'] = pos\n",
    "        \n",
    "    # print('sts_gold len={}, neutrals={}, {:.2f}%'.format(len(sts_gold_pos),empty_lists,(empty_lists/len(sts_gold_pos))*100))\n",
    "    # print(pos_tags_found)\n",
    "    \n",
    "    # print(sts_gold_pos.head())  \n",
    "    return sts_gold_pos\n",
    "\n",
    "sts_gold_pos = preprocess_pos(sts_gold_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering using synsets to check for adjectives in related words, (not in final model, gave unsatisfactory results on cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467933112</td>\n",
       "      <td>0</td>\n",
       "      <td>[is, going, miss]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2323395086</td>\n",
       "      <td>0</td>\n",
       "      <td>[looks, is, getting, traded, cleveland, play, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467968979</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, april, coming, soon, enough]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990283756</td>\n",
       "      <td>0</td>\n",
       "      <td>[drinking, mcdonalds, not, understanding, hurt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988884918</td>\n",
       "      <td>0</td>\n",
       "      <td>[so, dissapointed, have]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1467984364</td>\n",
       "      <td>0</td>\n",
       "      <td>[was, on, fling, sigh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1964951623</td>\n",
       "      <td>0</td>\n",
       "      <td>[got, got, just, left, work]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1881133744</td>\n",
       "      <td>0</td>\n",
       "      <td>[only, been, i, miss, especially, @, ktjade]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1753802024</td>\n",
       "      <td>0</td>\n",
       "      <td>[not, working, again]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1980497384</td>\n",
       "      <td>0</td>\n",
       "      <td>[lebron]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1755405128</td>\n",
       "      <td>0</td>\n",
       "      <td>[sucks, sometimes, theres, super, adorable, ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1966205498</td>\n",
       "      <td>0</td>\n",
       "      <td>[has, broken]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2206224118</td>\n",
       "      <td>0</td>\n",
       "      <td>[was, too, long, so, i, t, get, on]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1687790990</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, is, going, throw, up, high]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1963812485</td>\n",
       "      <td>0</td>\n",
       "      <td>[scratched]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1467844505</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, really, don]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2258491381</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, is, too, love, go, see, again, sadly, wont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1468039375</td>\n",
       "      <td>0</td>\n",
       "      <td>[hate, genuinely, excited, last]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2212063772</td>\n",
       "      <td>0</td>\n",
       "      <td>[s, sick, nausious, nasal, is, running, theraflu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1557163971</td>\n",
       "      <td>0</td>\n",
       "      <td>[not, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1975133437</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, wish, was, like, on, no]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1932575931</td>\n",
       "      <td>0</td>\n",
       "      <td>[no, lost, are, away, triggering, nuclear, mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1467899025</td>\n",
       "      <td>0</td>\n",
       "      <td>[still, sick, feeling, better, got, some, new,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2324113552</td>\n",
       "      <td>0</td>\n",
       "      <td>[worked, on, all]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1468042132</td>\n",
       "      <td>0</td>\n",
       "      <td>[was]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1467985185</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1980282216</td>\n",
       "      <td>0</td>\n",
       "      <td>[english, is, banjaxed, is, just, not]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2179698078</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, too, bad, seattle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1467985259</td>\n",
       "      <td>0</td>\n",
       "      <td>[arent, going]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2299794306</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, seattlerealest8, fat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>2194433506</td>\n",
       "      <td>0</td>\n",
       "      <td>[m, t, new, is, shipping, ....]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>1978986441</td>\n",
       "      <td>0</td>\n",
       "      <td>[seeing, s, latest, i, wish, was, london, righ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>1467912100</td>\n",
       "      <td>0</td>\n",
       "      <td>[carolrainbow, no, home, have, drive, use]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>1467872759</td>\n",
       "      <td>0</td>\n",
       "      <td>[lonely, keep, female]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>1932409653</td>\n",
       "      <td>0</td>\n",
       "      <td>[damn, lost, lost, sum]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>1982476349</td>\n",
       "      <td>0</td>\n",
       "      <td>[t, finish, fifty, more, go, want, play]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>1467858627</td>\n",
       "      <td>0</td>\n",
       "      <td>[just, woke, up, already, have, written, some,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>2199414669</td>\n",
       "      <td>0</td>\n",
       "      <td>[got, t, kind]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>2013743788</td>\n",
       "      <td>0</td>\n",
       "      <td>[leehopkins, are, just, too, adorable, have, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>2241033723</td>\n",
       "      <td>0</td>\n",
       "      <td>[really, want, i, have, pay, few, off, first, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>1795305046</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, t, be, too, bummed, saw, imax, largest, fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>1467910986</td>\n",
       "      <td>0</td>\n",
       "      <td>[keeping, crossed, is, not, feeling, well]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>1468038099</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, sudam08, been, reading, properly]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>2053385413</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, miss, so, much, sosososo, much, i, wanna, be]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>2000663077</td>\n",
       "      <td>0</td>\n",
       "      <td>[just, waking, up, feeling, really, tired, doe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>1467879203</td>\n",
       "      <td>0</td>\n",
       "      <td>[just, saw, found, fucking, terrible]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>2057515232</td>\n",
       "      <td>0</td>\n",
       "      <td>[watching, uruguay, need]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>1754305167</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, couldn, t, buy, '#, t, even, afford]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>2209465938</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, like, install, is, huge]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>1880171965</td>\n",
       "      <td>0</td>\n",
       "      <td>[just, realised, have, @, s, t, now]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>2214025188</td>\n",
       "      <td>0</td>\n",
       "      <td>[hope, have, fixed, xboxlive, last, was, havin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>1467986976</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, unfortunately, wasn, t, giant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>1752634162</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, hit, up, downtown, went, down, pike, was, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>1467871661</td>\n",
       "      <td>0</td>\n",
       "      <td>[miss, broke, now, i, using, stupid, ughhh, i,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>2071726448</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, said, be, .., banging, on]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>1468049681</td>\n",
       "      <td>0</td>\n",
       "      <td>[sized, is, nice, sad, lonely, no, puppy, kitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>2195475499</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, hurry, up, home, dying, no]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>1996172176</td>\n",
       "      <td>0</td>\n",
       "      <td>[lol, only, had, lost, amp, theres]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>2016105580</td>\n",
       "      <td>4</td>\n",
       "      <td>[good, is, such, beautiful, here, new]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>2186977170</td>\n",
       "      <td>4</td>\n",
       "      <td>[was, @, won, is, mvp, just, thought, i, tell]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2034 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467933112</td>\n",
       "      <td>0</td>\n",
       "      <td>[is, going, miss]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2323395086</td>\n",
       "      <td>0</td>\n",
       "      <td>[looks, is, getting, traded, cleveland, play, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467968979</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, april, coming, soon, enough]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990283756</td>\n",
       "      <td>0</td>\n",
       "      <td>[drinking, mcdonalds, not, understanding, hurt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988884918</td>\n",
       "      <td>0</td>\n",
       "      <td>[so, dissapointed, have]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1467984364</td>\n",
       "      <td>0</td>\n",
       "      <td>[was, on, fling, sigh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1964951623</td>\n",
       "      <td>0</td>\n",
       "      <td>[got, got, just, left, work]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1881133744</td>\n",
       "      <td>0</td>\n",
       "      <td>[only, been, i, miss, especially, @, ktjade]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1753802024</td>\n",
       "      <td>0</td>\n",
       "      <td>[not, working, again]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1980497384</td>\n",
       "      <td>0</td>\n",
       "      <td>[lebron]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1755405128</td>\n",
       "      <td>0</td>\n",
       "      <td>[sucks, sometimes, theres, super, adorable, ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1966205498</td>\n",
       "      <td>0</td>\n",
       "      <td>[has, broken]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2206224118</td>\n",
       "      <td>0</td>\n",
       "      <td>[was, too, long, so, i, t, get, on]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1687790990</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, is, going, throw, up, high]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1963812485</td>\n",
       "      <td>0</td>\n",
       "      <td>[scratched]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1467844505</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, really, don]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2258491381</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, is, too, love, go, see, again, sadly, wont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1468039375</td>\n",
       "      <td>0</td>\n",
       "      <td>[hate, genuinely, excited, last]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2212063772</td>\n",
       "      <td>0</td>\n",
       "      <td>[s, sick, nausious, nasal, is, running, theraflu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1557163971</td>\n",
       "      <td>0</td>\n",
       "      <td>[not, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1975133437</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, wish, was, like, on, no]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1932575931</td>\n",
       "      <td>0</td>\n",
       "      <td>[no, lost, are, away, triggering, nuclear, mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1467899025</td>\n",
       "      <td>0</td>\n",
       "      <td>[still, sick, feeling, better, got, some, new,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2324113552</td>\n",
       "      <td>0</td>\n",
       "      <td>[worked, on, all]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1468042132</td>\n",
       "      <td>0</td>\n",
       "      <td>[was]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1467985185</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1980282216</td>\n",
       "      <td>0</td>\n",
       "      <td>[english, is, banjaxed, is, just, not]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2179698078</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, too, bad, seattle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1467985259</td>\n",
       "      <td>0</td>\n",
       "      <td>[arent, going]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2299794306</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, seattlerealest8, fat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>2194433506</td>\n",
       "      <td>0</td>\n",
       "      <td>[m, t, new, is, shipping, ....]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>1978986441</td>\n",
       "      <td>0</td>\n",
       "      <td>[seeing, s, latest, i, wish, was, london, righ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>1467912100</td>\n",
       "      <td>0</td>\n",
       "      <td>[carolrainbow, no, home, have, drive, use]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>1467872759</td>\n",
       "      <td>0</td>\n",
       "      <td>[lonely, keep, female]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>1932409653</td>\n",
       "      <td>0</td>\n",
       "      <td>[damn, lost, lost, sum]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>1982476349</td>\n",
       "      <td>0</td>\n",
       "      <td>[t, finish, fifty, more, go, want, play]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>1467858627</td>\n",
       "      <td>0</td>\n",
       "      <td>[just, woke, up, already, have, written, some,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>2199414669</td>\n",
       "      <td>0</td>\n",
       "      <td>[got, t, kind]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>2013743788</td>\n",
       "      <td>0</td>\n",
       "      <td>[leehopkins, are, just, too, adorable, have, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>2241033723</td>\n",
       "      <td>0</td>\n",
       "      <td>[really, want, i, have, pay, few, off, first, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>1795305046</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, t, be, too, bummed, saw, imax, largest, fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>1467910986</td>\n",
       "      <td>0</td>\n",
       "      <td>[keeping, crossed, is, not, feeling, well]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>1468038099</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, sudam08, been, reading, properly]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>2053385413</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, miss, so, much, sosososo, much, i, wanna, be]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>2000663077</td>\n",
       "      <td>0</td>\n",
       "      <td>[just, waking, up, feeling, really, tired, doe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>1467879203</td>\n",
       "      <td>0</td>\n",
       "      <td>[just, saw, found, fucking, terrible]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>2057515232</td>\n",
       "      <td>0</td>\n",
       "      <td>[watching, uruguay, need]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>1754305167</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, couldn, t, buy, '#, t, even, afford]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>2209465938</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, like, install, is, huge]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>1880171965</td>\n",
       "      <td>0</td>\n",
       "      <td>[just, realised, have, @, s, t, now]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>2214025188</td>\n",
       "      <td>0</td>\n",
       "      <td>[hope, have, fixed, xboxlive, last, was, havin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>1467986976</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, unfortunately, wasn, t, giant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>1752634162</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, hit, up, downtown, went, down, pike, was, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>1467871661</td>\n",
       "      <td>0</td>\n",
       "      <td>[miss, broke, now, i, using, stupid, ughhh, i,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>2071726448</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, said, be, .., banging, on]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>1468049681</td>\n",
       "      <td>0</td>\n",
       "      <td>[sized, is, nice, sad, lonely, no, puppy, kitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>2195475499</td>\n",
       "      <td>0</td>\n",
       "      <td>[@, hurry, up, home, dying, no]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>1996172176</td>\n",
       "      <td>0</td>\n",
       "      <td>[lol, only, had, lost, amp, theres]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>2016105580</td>\n",
       "      <td>4</td>\n",
       "      <td>[good, is, such, beautiful, here, new]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>2186977170</td>\n",
       "      <td>4</td>\n",
       "      <td>[was, @, won, is, mvp, just, thought, i, tell]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2034 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_adj(word):\n",
    "    synsets = wordnet.synsets(word)\n",
    "    for ss in synsets:\n",
    "        if ss.pos() in ['a','r','v'] :\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "[word for word in wordpunct_tokenize(\"@mikefish  Fair enough. But i have the Kindle2 and I think it's perfect  :)\") if check_adj(word)]\n",
    "\n",
    "wordnet.synsets('enough')\n",
    "sts_gold_pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_remove_stopwords(input_dataset):\n",
    "    sts_gold_no_stopwords = input_dataset.copy()\n",
    "    s_words = set(stopwords.words('english'))\n",
    "    s_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}','@'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # sts_gold_tokenized.drop('tweet',axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "    for i,row in input_dataset.iterrows():\n",
    "        sts_gold_no_stopwords.at[i, 'tweet'] = [word for word in row['tweet'] if not (word in s_words or len(word) < 2)]\n",
    "     \n",
    "    empty_lists = 0\n",
    "    for i,row in sts_gold_no_stopwords.iterrows():\n",
    "        if sts_gold_no_stopwords.at[i, 'tweet'] == []:\n",
    "            empty_lists += 1\n",
    "    # print('sts_gold len={}, neutrals={}, {:.2f}%'.format(len(input_dataset),empty_lists,(empty_lists/len(input_dataset))*100))\n",
    "    \n",
    "        \n",
    "    # print(sts_gold_no_stopwords.head(20))\n",
    "    return sts_gold_no_stopwords\n",
    "\n",
    "sts_gold_no_stopwords = preprocess_remove_stopwords(sts_gold_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing (not in final model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lemmatize(input_dataset):\n",
    "    sts_gold_lemmatized = input_dataset.copy()\n",
    "    wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # print(wordnet_lemmatizer.lemmatize(\"churches\"))\n",
    "    \n",
    "    \n",
    "    for i,row in input_dataset.iterrows():\n",
    "        sts_gold_lemmatized.at[i, 'tweet'] = [wordnet_lemmatizer.lemmatize(word) for word in row['tweet']]\n",
    "        # if not (sts_gold_lemmatized.at[i,'tweet'] == sts_gold_no_stopwords.at[i,'tweet']):\n",
    "        #     print(\"pre-lemmatized {}\\npost-lemmatized {}\".format(sts_gold_no_stopwords.at[i,'tweet'],sts_gold_lemmatized.at[i,'tweet']))\n",
    "     \n",
    "    empty_lists = 0\n",
    "    for i,row in sts_gold_lemmatized.iterrows():\n",
    "        if sts_gold_lemmatized.at[i, 'tweet'] == []:\n",
    "            empty_lists += 1\n",
    "            sts_gold_lemmatized.at[i,'polarity'] = 2\n",
    "    # print('sts_gold len={}, neutrals={}, {:.2f}%'.format(len(sts_gold_pos),empty_lists,(empty_lists/len(sts_gold_pos))*100))\n",
    "    \n",
    "        \n",
    "    # print(sts_gold_lemmatized.head(20))\n",
    "    return sts_gold_lemmatized\n",
    "\n",
    "sts_gold_lemmatized = preprocess_stemming(sts_gold_no_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our classifier should consider tweets without allowed POS-tags to be neutral\n",
    "so no adjectives for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_make_neutral(input_dataset):\n",
    "    sts_gold_neutral = input_dataset.copy()\n",
    "\n",
    "    for i,row in input_dataset.iterrows():\n",
    "        if sts_gold_neutral.at[i, 'tweet'] == []:\n",
    "            sts_gold_neutral.at[i,'polarity'] = 2\n",
    "\n",
    "    return sts_gold_neutral\n",
    "\n",
    "sts_gold_neutral = preprocess_make_neutral(sts_gold_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all(input_dataset):\n",
    "    return preprocess_make_neutral(preprocess_stemming(preprocess_remove_stopwords(preprocess_pos(preprocess_tokenize(input_dataset)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "### training the classifier and creating the features (bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n        contains(awesom) = True           positi : negati =     31.5 : 1.0\n        contains(beauti) = True           positi : negati =     15.4 : 1.0\n          contains(love) = True           positi : negati =     14.3 : 1.0\n          contains(glad) = True           positi : negati =     12.4 : 1.0\n          contains(sick) = True           negati : positi =     11.7 : 1.0\n          contains(amaz) = True           positi : negati =     11.0 : 1.0\n         contains(funni) = True           positi : negati =      9.5 : 1.0\n         contains(excit) = True           positi : negati =      8.5 : 1.0\n        contains(person) = True           positi : negati =      8.0 : 1.0\n         contains(magic) = True           positi : negati =      8.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for i, row in sts_gold_neutral.iterrows():\n",
    "    all_words.extend(row['tweet'])\n",
    "all_words = list(set(all_words)) #remove duplicates\n",
    "\n",
    "def contains_feature(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in all_words:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features\n",
    "\n",
    "\n",
    "sts_gold_shuffled = sts_gold_neutral.sample(frac=1)\n",
    "sts_gold_shuffled = sts_gold_shuffled[sts_gold_shuffled['polarity'] != 2]\n",
    "\n",
    "def polarity_mapping(numb):\n",
    "    if (numb == 0):\n",
    "        return 'negative'\n",
    "    elif (numb == 2):\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'\n",
    "\n",
    "training_set = nltk.classify.apply_features(contains_feature,\n",
    "[(row['tweet'],polarity_mapping(row['polarity'])) for i,row in sts_gold_shuffled.iterrows()],labeled=True)\n",
    "\n",
    "\n",
    "clf = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "clf.show_most_informative_features()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classify custom tweets/youtube comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['negative',\n 'positive',\n 'negative',\n 'positive',\n 'negative',\n 'negative',\n 'positive']"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutrals = 0\n",
    "for dat in training_set:\n",
    "    if dat[1] == 'neutral':\n",
    "        neutrals += 1\n",
    "print(len(training_set),neutrals)\n",
    "\n",
    "dat = {\n",
    "    'tweet' : ['bad things happen to terrible people','i like awesome stuff','so sentimental','i love you','what a sad world we live in',\n",
    "               'i liked kiwi and corolina of his songs', 'amazing']\n",
    "}\n",
    "dat = pd.DataFrame(dat)\n",
    "dat_pre = preprocess_all(dat)\n",
    "dat_feat = [contains_feature(sent) for sent in dat_pre['tweet']]\n",
    "\n",
    "clf.classify_many(dat_feat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossvalidation\n",
    "With some multithreading that didn't improve the speed that much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\naccuracy: 77.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\naccuracy: 81.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\naccuracy: 78.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\naccuracy: 82.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\naccuracy: 76.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\naccuracy: 78.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\naccuracy: 77.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\naccuracy: 78.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\naccuracy: 78.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\naccuracy: 84.85\n\n\naverage accuracy: 79.50\n79.4964722603\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_cross_val_score(resval):\n",
    "\n",
    "    cv = cross_validation.KFold(len(training_set), n_folds=10, shuffle=True, random_state=None)\n",
    "    \n",
    "    from operator import itemgetter\n",
    "    \n",
    "    \n",
    "    \n",
    "    accuracies = []\n",
    "    for traincv, testcv in cv:\n",
    "        # print(\"train indices: {} to {}\\ntest indices: {} to {}\".format(traincv[0],traincv[len(traincv)-1],testcv[0],testcv[len(testcv)-1]))\n",
    "        classifier = nltk.NaiveBayesClassifier.train(itemgetter(*traincv)(training_set))\n",
    "        accuracy = nltk.classify.util.accuracy(classifier, itemgetter(*testcv)(training_set))\n",
    "        print('\\naccuracy: {:.02f}'.format(accuracy*100))\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    print('\\n\\naverage accuracy: {:.02f}'.format(np.mean(accuracies)*100))\n",
    "    return np.mean(accuracies)*100\n",
    "    \n",
    "    \n",
    "# from multiprocessing.dummy import Pool as ThreadPool \n",
    "# pool = ThreadPool(3) \n",
    "# average_accuracies = [0]*3\n",
    "# results = pool.map(get_cross_val_score, average_accuracies)\n",
    "# print(results)\n",
    "\n",
    "print(get_cross_val_score(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synsets testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('good.n.01') Synset('amazing.s.01')\n0.125\nNone\nNone\nNone\nNone\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "# print(wns.word_similarity('true','good',\"wup\"))\n",
    "g1 = wordnet.synset('good.n.01')\n",
    "g2 = wordnet.synset('amazing.s.01')\n",
    "print(g1,g2)\n",
    "for g2 in wordnet.synsets('super'):\n",
    "    print(g1.wup_similarity(g2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparing results on the sentiment140 test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177 0 182 359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359 0\npositive positive positive @stellargirl I loooooooovvvvvveee my Kindle2. Not that the DX is cool, but the 2 is fantastic in its own right. ['loooooooovvvvvvee', 'cool', 'fantast']\npositive positive positive Reading my kindle2...  Love it... Lee childs is good read. ['read', 'love', 'lee', 'good']\nnegative negative positive Ok, first assesment of the #kindle2 ...it fucking rocks!!! ['first', 'fuck', '!!!']\npositive negative positive @kenburbary You'll love your Kindle2. I've had mine for a few months and never looked back. The new big one is huge! No need for remorse! :) ['never', 'look', 'back', 'new', 'big', 'huge']\npositive negative positive @mikefish  Fair enough. But i have the Kindle2 and I think it's perfect  :) ['mikefish', 'enough', 'think']\npositive positive positive @richardebaker no. it is too big. I'm quite happy with the Kindle2. ['big', 'quit', 'happi']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative negative negative Fuck this economy. I hate aig and their non loan given asses. ['fuck', 'hate', 'given']\npositive positive positive Jquery is my new best friend. ['new', 'best']\nneutral negative positive Loves twitter []\nnegative positive positive how can you not love Obama? he makes jokes about himself. ['love', 'obama', 'make']\nnegative negative negative @Karoli I firmly believe that Obama/Pelosi have ZERO desire to be civil.  It's a charade and a slogan, but they want to destroy conservatism ['firmli', 'believ', 'zero', 'civil', 'want', 'destroy']\nneutral negative positive House Correspondents dinner was last night whoopi, barbara &amp; sherri went, Obama got a standing ovation ['last', 'went', 'got']\npositive positive positive Watchin Espn..Jus seen this new Nike Commerical with a Puppet Lebron..sh*t was hilarious...LMAO!!! ['watchin', 'seen', 'new', 'nike', 'hilari']\nnegative negative negative dear nike, stop with the flywire. that shit is a waste of science. and ugly. love, @vincentx24x ['stop', 'ugli', 'love']\npositive positive positive #lebron best athlete of our generation, if not all time (basketball related) I don't want to get into inter-sport debates about   __1/2 ['best', 'relat', 'want', 'get', 'inter']\nnegative negative negative I was talking to this guy last night and he was telling me that he is a die hard Spurs fan.  He also told me that he hates LeBron James. ['talk', 'last', 'tell', 'die', 'hard', 'also', 'told', 'hate', 'lebron']\npositive positive positive i love lebron. http://bit.ly/PdHur ['love', '://', 'ly']\nneutral negative negative @ludajuice Lebron is a Beast, but I'm still cheering 4 the A..til the end. ['still', 'cheer', '..']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral negative positive @Pmillzz lebron IS THE BOSS []\npositive positive positive @sketchbug Lebron is a hometown hero to me, lol I love the Lakers but let's go Cavs, lol ['hometown', 'lol', 'love', 'let', 'go']\npositive positive positive lebron and zydrunas are such an awesome duo ['awesom']\nneutral negative positive @wordwhizkid Lebron is a beast... nobody in the NBA comes even close. ['come', 'even', 'close']\npositive negative positive downloading apps for my iphone! So much fun :-) There literally is an app for just about anything. ['download', 'much', 'liter']\nnegative positive positive good news, just had a call from the Visa office, saying everything is fine.....what a relief! I am sick of scams out there! Stealing! ['good', 'say', 'fine']\npositive positive positive http://twurl.nl/epkr4b - awesome come back from @biz (via @fredwilson) ['nl', 'awesom', 'come', 'back']\npositive negative positive In montreal for a long weekend of R&amp;R. Much needed. ['long', 'much', 'need']\nnegative negative positive Booz Allen Hamilton has a bad ass homegrown social collaboration platform. Way cool!  #ttiv ['allen', 'bad', 'homegrown', 'social', 'cool']\nneutral negative positive [#MLUC09] Customer Innovation Award Winner: Booz Allen Hamilton -- http://ping.fm/c2hPP ['[#', 'http', '://', 'fm']\npositive positive positive @SoChi2 I current use the Nikon D90 and love it, but not as much as the Canon 40D/50D. I chose the D90 for the  video feature. My mistake. ['current', 'love', 'much', 'chose']\npositive positive positive @phyreman9 Google is always a good place to look. Should've mentioned I worked on the Mustang w/ my Dad, @KimbleT. ['alway', 'good', 'look', 'mention', 'work']\npositive negative negative Played with an android google phone. The slide out screen scares me I would break that fucker so fast. Still prefer my iPhone. ['play', 'android', 'break', 'fast', 'still', 'prefer']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative negative negative US planning to resume the military tribunals at Guantanamo Bay... only this time those on trial will be AIG execs and Chrysler debt holders ['plan', 'resum', 'militari', 'aig', 'chrysler']\nnegative negative negative omg so bored &amp; my tattoooos are so itchy!!  help! aha =) ['bore', 'itchi']\nnegative negative negative I'm itchy and miserable! ['miser']\npositive negative negative @sekseemess no. I'm not itchy for now. Maybe later, lol. ['itchi', 'mayb', 'later']\npositive positive positive RT @jessverr I love the nerdy Stanford human biology videos - makes me miss school. http://bit.ly/13t7NR ['love', 'nerdi', 'human', 'make', 'miss', '://', 'ly']\nnegative positive positive @spinuzzi: Has been a bit crazy, with steep learning curve, but LyX is really good for long docs. For anything shorter, it would be insane. ['crazi', 'steep', 'learn', 'realli', 'good', 'long']\npositive positive positive I'm listening to \"P.Y.T\" by Danny Gokey &lt;3 &lt;3 &lt;3 Aww, he's so amazing. I &lt;3 him so much :) ['listen', 'amaz', 'much']\npositive negative positive is going to sleep then on a bike ride:] ['go', 'sleep']\nneutral negative negative cant sleep... my tooth is aching. ['cant', 'ach']\npositive negative negative Blah, blah, blah same old same old. No plans today, going back to sleep I guess. ['old', 'old', 'go', 'back', 'sleep']\nneutral negative negative glad i didnt do Bay to Breakers today, it's 1000 freaking degrees in San Francisco wtf ['didnt', 'bay', 'freak', 'san']\nneutral negative negative ?Obama Administration Must Stop Bonuses to AIG Ponzi Schemers ... http://bit.ly/2CUIg ['obama', 'stop', 'aig', 'http', '://', 'ly']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral positive negative started to think that Citi is in really deep s&amp;^t. Are they gonna survive the turmoil or are they gonna be the next AIG? ['start', 'think', 'realli', 'deep', 'gonna', 'surviv', 'gonna', 'next']\n64.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\naccuracy: 70.19\n"
     ]
    }
   ],
   "source": [
    "cv = cross_validation.KFold(len(training_set), n_folds=10, shuffle=True, random_state=None)\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "header = ['polarity','tweet_id','time','query','user','tweet']\n",
    "sent140_test = pd.read_csv('sentiment140/testdata.manual.2009.06.14.csv',\n",
    "                            error_bad_lines=False, encoding='latin-1',names=header,header=None)\n",
    "\n",
    "sent140_test = sent140_test[sent140_test['polarity'] != 2]\n",
    "\n",
    "print(len(sent140_test[sent140_test['polarity'] == 0]),len(sent140_test[sent140_test['polarity'] == 2]),len(sent140_test[sent140_test['polarity'] == 4]),len(sent140_test))\n",
    "\n",
    "sent140_feat = preprocess_all(sent140_test)\n",
    "\n",
    "print(len(sent140_test),len(sent140_test[sent140_test['polarity'] == 2]))\n",
    "\n",
    "def polarity_mapping(numb):\n",
    "    if (numb == 0):\n",
    "        return 'negative'\n",
    "    elif (numb == 2):\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'\n",
    "\n",
    "\n",
    "test_set = nltk.classify.apply_features(contains_feature,\n",
    "[(row['tweet'],polarity_mapping(row['polarity'])) for i,row in sent140_feat.iterrows()],labeled=True)\n",
    "\n",
    "\n",
    "\n",
    "errors = 0\n",
    "for i, row in sent140_test.iterrows():\n",
    "    tb = TextBlob(text=row['tweet'])\n",
    "    pol_tb = tb.polarity\n",
    "    if (pol_tb < 0):\n",
    "        pol_tb = 'negative'\n",
    "    elif (pol_tb > 0):\n",
    "        pol_tb = 'positive'\n",
    "    else:\n",
    "        pol_tb = 'neutral'\n",
    "    tweet_data = pd.DataFrame({'tweet' : [row['tweet']]});\n",
    "    pol_gt = polarity_mapping(row['polarity'])\n",
    "    pol_clf = clf.classify(contains_feature(preprocess_all(tweet_data).at[0,'tweet']));\n",
    "    \n",
    "    # if (pol_gt != polarity_mapping(row['polarity'])):\n",
    "    if (i < 50):\n",
    "        print(pol_tb,pol_clf,pol_gt,row['tweet'],preprocess_all(tweet_data).at[0,'tweet'])\n",
    "     \n",
    "    if not pol_gt == pol_tb:\n",
    "        errors += 1\n",
    "        if (i < 500):\n",
    "            pass\n",
    "            # print(pol_tb,pol_clf,row['polarity'],row['tweet'],sts_gold_neutral.at[i,'tweet'])\n",
    "print(\"{:.02f}\".format(100-(errors/len(sent140_test))*100))\n",
    "\n",
    "# print(\"train indices: {} to {}\\ntest indices: {} to {}\".format(traincv[0],traincv[len(traincv)-1],testcv[0],testcv[len(testcv)-1]))\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "accuracy = nltk.classify.util.accuracy(classifier, test_set)\n",
    "print('\\naccuracy: {:.02f}'.format(accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive positive 4 @JBsFanArgentina Hey I luv this pic!!! was amazing of the last CHAT of The JB in FACEBOOK!  ['luv', 'amaz', 'last']\npositive positive 4 I L&lt;3VE Taylor Swift.She is just so amazing and her music is wonderfull and makes me so happy.  ['amaz', 'wonderful', 'make', 'happi']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive positive 4 watching you belong to mee video  --taylor swift is amazing, love her...my new idol lol  ['watch', 'belong', 'mee', 'amaz', 'love', 'new']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive positive 4 @pcdnicole Hi Nicole, glad you loved Sydney! Was amazing working with you the other night at 301! Looking forward to you coming back  ['love', 'amaz', 'work', 'look', 'forward', 'come', 'back']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive positive 4 @_Chelsea_Marie does target ship things to london? thanks so much! im such a demi fan shes amazing!  ['target', 'ship', 'london', 'much', 'amaz']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive positive 4 Whirlpool Galaxy Deep Field  : http://apod.nasa.gov/apod/ap090526.html  what an amazing universe  ['galaxi', 'deep', 'html', 'amaz']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive positive 4 hung out by Notre Dame today hoping  for a glimpse of Obama...and I got it!   the energy was amazing! ['notr', 'hope', 'got', 'amaz']\npositive positive 4 Youtube comedy people are just amazing.  ['youtub', 'amaz']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive positive 4 its amazing how a starbucks caramel frappucino can relieve stress..itz seriously an exordianry drink..am feelin much the better now!  ['amaz', 'caramel', 'reliev', 'serious', 'feelin', 'much', 'better']\npositive positive 4 @ddlovato yesterday &quot;sonny with a chance&quot; came to brazil, i loved it! you're amazing &lt;33 please reply  ['came', 'brazil', 'love', 'amaz']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive positive 4 @sydney_sider yes thanks I think they're amazing too! the images were taken by @insidecuisine photographer the very talented @rovingrob  ['sydney_sid', 'ye', 'think', 'amaz', 'taken', 'talent']\npositive positive 4 and now off to bed after an amazing night chatting with a pretty amazing guy  ( you know who you are) ['amaz', 'chat', 'pretti', 'amaz', 'know']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive positive 4 @adeline_sky  that sounds fantastic! You're amazing! We need to watch some Muse gigs too! Shall we do it Saturday night? ['sound', 'fantast', 'need', 'watch', 'saturday']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive positive 4 @DonnieWahlberg YOU give me joy....and you have for years. You are such an amazing man and we are all lucky to know you.  ['give', 'amaz', 'lucki', 'know']\n1052 2034 48.279252704031464\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf_polarities = clf.classify_many([contains_feature(sent) for sent in preprocess_all(sts_gold)])\n",
    "\n",
    "\n",
    "errors = 0\n",
    "for i, row in sts_gold.iterrows():\n",
    "    tb = TextBlob(text=row['tweet'])\n",
    "    pol_tb = tb.polarity\n",
    "    if (pol_tb < 0):\n",
    "        pol_tb = 'negative'\n",
    "    elif (pol_tb > 0):\n",
    "        pol_tb = 'positive'\n",
    "    else:\n",
    "        pol_tb = 'neutral'\n",
    "    tweet_data = pd.DataFrame({'tweet' : [row['tweet']]});\n",
    "    pol_clf = clf.classify(contains_feature(preprocess_all(tweet_data).at[0,'tweet']));\n",
    "    \n",
    "    if ('amazing' in row['tweet'] and row['polarity'] == 4):\n",
    "        print(pol_tb,pol_clf,row['polarity'],row['tweet'],sts_gold_neutral.at[i,'tweet'])\n",
    "     \n",
    "    if not pol_clf == pol_tb:\n",
    "        errors += 1\n",
    "        if (i < 500):\n",
    "            pass\n",
    "            # print(pol_tb,pol_clf,row['polarity'],row['tweet'],sts_gold_neutral.at[i,'tweet'])\n",
    "    \n",
    "    \n",
    "print(errors,len(sts_gold),100-(errors/len(sts_gold))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing training our classifier on a huge (automatically labelled) training set\n",
    "code works but the the classifier performance was lacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length= 1600000\ndataset length= 1600000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length= 1600000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2eb264e6a368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0msent140_train_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent140_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"preprocessing done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9040dc40f188>\u001b[0m in \u001b[0;36mpreprocess_all\u001b[0;34m(input_dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreprocess_lemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_remove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-6c89179c88f2>\u001b[0m in \u001b[0;36mpreprocess_tokenize\u001b[0;34m(input_dataset)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0msts_gold_tokenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordpunct_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dvpeer/Documents/TuDelft/InformationRetrieval/GroupProject/sentiment/venv/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dvpeer/Documents/TuDelft/InformationRetrieval/GroupProject/sentiment/venv/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                 data = _sanitize_array(data, index, dtype, copy,\n\u001b[0;32m--> 264\u001b[0;31m                                        raise_cast_failure=True)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dvpeer/Documents/TuDelft/InformationRetrieval/GroupProject/sentiment/venv/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m   3193\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3194\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3195\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dvpeer/Documents/TuDelft/InformationRetrieval/GroupProject/sentiment/venv/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_try_cast\u001b[0;34m(arr, take_fast_path)\u001b[0m\n\u001b[1;32m   3159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3161\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_cast_to_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3162\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_extension_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dvpeer/Documents/TuDelft/InformationRetrieval/GroupProject/sentiment/venv/lib/python3.5/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mmaybe_cast_to_datetime\u001b[0;34m(value, dtype, errors)\u001b[0m\n\u001b[1;32m    937\u001b[0m     \u001b[0mnan\u001b[0m \u001b[0mto\u001b[0m \u001b[0miNaT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \"\"\"\n\u001b[0;32m--> 939\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedeltas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_timedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetimes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "header = ['polarity','tweet_id','time','query','user','tweet']\n",
    "sent140_train = pd.read_csv('sentiment140/training.1600000.processed.noemoticon.csv',\n",
    "                            error_bad_lines=False, encoding='latin-1',names=header,header=None)\n",
    "# print(sent140_train.head())\n",
    "\n",
    "print(\"dataset length=\",len(sent140_train))\n",
    "\n",
    "sent140_train = sent140_train[sent140_train.polarity != 2]\n",
    "print(\"dataset length=\",len(sent140_train))\n",
    "sent140_train = sent140_train.sample(frac=1)\n",
    "\n",
    "print(\"dataset length=\",len(sent140_train))\n",
    "\n",
    "\n",
    "sent140_train_processed = preprocess_all(sent140_train)\n",
    "\n",
    "print(\"preprocessing done\")\n",
    "\n",
    "\n",
    "all_words = []\n",
    "for i, row in sent140_train_processed.iterrows():\n",
    "    all_words.extend(row['tweet'])\n",
    "all_words = list(set(all_words)) #remove duplicates\n",
    "\n",
    "def contains_feature(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in all_words:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features\n",
    "\n",
    "\n",
    "sent140_train_shuffled = sent140_train_processed.sample(frac=1)\n",
    "\n",
    "training_set = nltk.classify.apply_features(contains_feature,\n",
    "[(row['tweet'],'negative') if (row['polarity'] == 0 or row['polarity'] == 2) else (row['tweet'],'positive') for i,row in sent140_train_shuffled.iterrows()],labeled=True)\n",
    "\n",
    "\n",
    "clf = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "\n",
    "clf.show_most_informative_features(10)\n",
    "\n",
    "probas = clf.prob_classify(contains_feature(['awesome']))\n",
    "print(probas.prob('positive'),probas.prob('negative'))\n",
    "\n",
    "dat = {\n",
    "    'tweet' : ['bad things happen to terrible people','i like awesome stuff','so sentimental','i love you','what a sad world we live in','i liked kiwi and corolina of his songs']\n",
    "}\n",
    "dat = pd.DataFrame(dat)\n",
    "dat_pre = preprocess_all(dat)\n",
    "dat_feat = [contains_feature(sent) for sent in dat_pre['tweet']]\n",
    "\n",
    "\n",
    "probas_l = clf.classify_many(dat_feat)\n",
    "print(\"probas_l\" + str(probas_l))\n",
    "\n",
    "# for probas in probas_l:\n",
    "#     print(probas.prob('positive'),probas.prob('negative'))\n",
    "cv = cross_validation.KFold(len(training_set), n_folds=10, shuffle=True, random_state=None)\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "\n",
    "accuracies = []\n",
    "for traincv, testcv in cv:\n",
    "    # print(\"train indices: {} to {}\\ntest indices: {} to {}\".format(traincv[0],traincv[len(traincv)-1],testcv[0],testcv[len(testcv)-1]))\n",
    "    classifier = nltk.NaiveBayesClassifier.train(itemgetter(*traincv)(training_set))\n",
    "    accuracy = nltk.classify.util.accuracy(classifier, itemgetter(*testcv)(training_set))\n",
    "    print('\\naccuracy: {:.02f}'.format(accuracy*100))\n",
    "    accuracies.append(accuracy)\n",
    "print('\\n\\naverage accuracy: {:.02f}'.format(np.mean(accuracies)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More detailed analysis and comparison to textblob on testset\n",
    "including recall and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Recall 79.12\nPositive Precision 61.28\nNegative Recall 48.59\nNegative Precision 69.35\nAccuracy 64.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\naccuracy: 70.75\n"
     ]
    }
   ],
   "source": [
    "header = ['polarity','tweet_id','time','query','user','tweet']\n",
    "sent140_test = pd.read_csv('sentiment140/testdata.manual.2009.06.14.csv',\n",
    "                            error_bad_lines=False, encoding='latin-1',names=header,header=None)\n",
    "\n",
    "sent140_test = sent140_test[sent140_test['polarity'] != 2]\n",
    "\n",
    "sent140_test_pos = sent140_test[sent140_test['polarity'] == 0]\n",
    "\n",
    "sent140_feat = preprocess_all(sent140_test)\n",
    "\n",
    "\n",
    "\n",
    "def polarity_mapping(numb):\n",
    "    if (numb == 0):\n",
    "        return 'negative'\n",
    "    elif (numb == 2):\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'\n",
    "\n",
    "\n",
    "test_set = nltk.classify.apply_features(contains_feature,\n",
    "[(row['tweet'],polarity_mapping(row['polarity'])) for i,row in sent140_feat.iterrows()],labeled=True)\n",
    "\n",
    "errors = 0\n",
    "positive = 0\n",
    "actually_positive = 0\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "for i, row in sent140_test.iterrows():\n",
    "    tb = TextBlob(text=row['tweet'])\n",
    "    pol_tb = tb.polarity\n",
    "    if (pol_tb < 0):\n",
    "        pol_tb = 'negative'\n",
    "    elif (pol_tb > 0):\n",
    "        pol_tb = 'positive'\n",
    "    else:\n",
    "        pol_tb = 'neutral'\n",
    "        \n",
    "    pol_gt = polarity_mapping(row['polarity'])\n",
    "        \n",
    "    tweet_data = pd.DataFrame({'tweet' : [row['tweet']]});\n",
    "    pol_clf = clf.classify(contains_feature(preprocess_all(tweet_data).at[0,'tweet']));\n",
    "    \n",
    "    \n",
    "    if (pol_gt == 'positive'):\n",
    "        if (pol_tb == 'positive'):\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    else:\n",
    "        if (pol_tb == 'negative'):\n",
    "            tn += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    \n",
    "# print(\"{:.02f}\".format(100-(errors/len(sent140_test_pos))*100))\n",
    "print(\"Positive Recall {:.02f}\\nPositive Precision {:.02f}\\nNegative Recall {:.02f}\\nNegative Precision {:.02f}\\nAccuracy {:.02f}\".format((tp/(tp+fn))*100,(tp/(tp+fp))*100,(tn/(fp+tn))*100,(tn/(tn+fn))*100,((tp+tn)/(tp+tn+fp+fn))*100))\n",
    "# print(\"train indices: {} to {}\\ntest indices: {} to {}\".format(traincv[0],traincv[len(traincv)-1],testcv[0],testcv[len(testcv)-1]))\n",
    "# classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "accuracy = nltk.classify.util.accuracy(clf, test_set)\n",
    "print('\\naccuracy: {:.02f}'.format(accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating the output datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 114463: expected 4 fields, saw 5\\n'\nb'Skipping line 142494: expected 4 fields, saw 8\\nSkipping line 189730: expected 4 fields, saw 6\\nSkipping line 245216: expected 4 fields, saw 7\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 388428: expected 4 fields, saw 5\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "691400"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_usa_comments= pd.read_csv(\"./youtube/UScomments.csv\",error_bad_lines=False)\n",
    "len(df_usa_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['video_id', 'tweet', 'likes', 'replies'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "columns = df_usa_comments.columns.tolist()\n",
    "columns[1] = 'tweet'\n",
    "df_usa_comments.columns = columns\n",
    "print(df_usa_comments.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0 0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 6717 5916 2632 44.4895199459094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 13220 11851 5070 42.78119989874272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 19806 17801 7606 42.72793663277344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 26368 23739 10106 42.57129617928304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 33020 29701 12720 42.82684084710952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 39649 35568 15216 42.7800269905533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000 46239 41453 17691 42.67724893252599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000 52794 47310 20103 42.49207355738744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000 59421 53119 22539 42.43114516463036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 66102 58955 25056 42.500212026121616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110000 72711 64920 27630 42.56007393715342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000 79400 70708 30107 42.57934038581207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130000 86180 76462 32641 42.68917893855772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000 92936 82242 35177 42.77254930570755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000 99713 88006 37718 42.85844146989978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160000 106548 93674 40221 42.937207763093284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170000 113380 99386 42765 43.02919928360131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000 120117 105111 45227 43.027846752480706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190000 126518 111263 47780 42.94329651366582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 133178 116951 50128 42.86239536216022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210000 139672 123031 52702 42.83635831619673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220000 146310 128849 55158 42.80824841481113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230000 152811 134882 57692 42.772200886700965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240000 159370 140826 60195 42.744237569766945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000 165963 146851 62813 42.773287209484444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260000 172567 152826 65392 42.78853074738592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270000 179201 158711 67911 42.789094643723494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280000 185833 164640 70472 42.80369290573372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290000 192642 170393 73034 42.86208940508119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000 199407 176252 75658 42.92603771872092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310000 206007 182123 78129 42.89902977657957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320000 212850 187761 80610 42.932238324252644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330000 219409 193766 83174 42.92497135720405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340000 226173 199603 85775 42.97280101000486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350000 232702 205705 88406 42.97707882647481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360000 239507 211533 91039 43.037729337739265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370000 246096 217656 93751 43.07301429779101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380000 252818 223532 96349 43.10300091262101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390000 259311 229661 98971 43.0943869442352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 265922 235693 101614 43.11286291913633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410000 272635 241616 104250 43.14697702138931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420000 279259 247531 106789 43.14166710432229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430000 285766 253557 109322 43.11535473286086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440000 292306 259475 111780 43.07929472974275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450000 298740 265530 114269 43.034308741008545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460000 305281 271443 116723 43.00092468768765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470000 312006 277324 119329 43.0287317361642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480000 318433 283359 121791 42.981165235619834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490000 325087 289146 124232 42.96514563576878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000 331617 295152 126768 42.95007318263132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510000 338145 300992 129136 42.90346587284712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520000 344736 306845 131580 42.88158516514853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "530000 351339 312842 134180 42.890660461191274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540000 357914 318642 136555 42.85530469931773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550000 364452 324664 139115 42.84891457014021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560000 370998 330437 141434 42.80210751217327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570000 377604 336432 144035 42.8125148618443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580000 384291 342153 146443 42.80044307663531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590000 390935 348033 148967 42.80255033287073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600000 397545 353816 151360 42.77929771406607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610000 404063 359846 153908 42.770518499580376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620000 410750 365571 156320 42.760503431617934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630000 417264 371551 158814 42.74352646070122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640000 423970 377317 161286 42.74548986661084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650000 430721 383059 163779 42.75555462735505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660000 437513 388718 166230 42.76364871191969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670000 444095 394513 168607 42.73800863342906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680000 450849 400198 171046 42.740343529952675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690000 457389 406141 173529 42.726294562725755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458324 66.28926815157651 173874 42.726133431625506\n"
     ]
    }
   ],
   "source": [
    "classified = []\n",
    "\n",
    "def reverrse_polarity_mapping(text):\n",
    "    if (text == 'negative'):\n",
    "        return 0\n",
    "    elif (text == 'neutral'):\n",
    "        return 2\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "def save_progress(classified):\n",
    "    if len(classified) < 10:\n",
    "        return \n",
    "    \n",
    "    output = pd.DataFrame(classified)\n",
    "    output = output[['video_id', 'comment', 'pol_textblob', 'pol_nlpWarriers']]\n",
    "    output.to_csv('output/all_data.csv')\n",
    "\n",
    "    output = pd.DataFrame([item for item in classified if 2 not in [item['pol_textblob'], item['pol_nlpWarriers']]])\n",
    "    output = output[['video_id', 'comment', 'pol_textblob', 'pol_nlpWarriers']]\n",
    "    output.to_csv('output/no_neutrals.csv')\n",
    "\n",
    "    output = pd.DataFrame([item for item in classified if (2 not in [item['pol_textblob'], item['pol_nlpWarriers']])\n",
    "                           and (item['pol_textblob'] == item['pol_nlpWarriers'])])\n",
    "    output = output[['video_id', 'comment', 'pol_textblob', 'pol_nlpWarriers']]\n",
    "    output.to_csv('output/nn_agree.csv')\n",
    "    \n",
    "    output = pd.DataFrame([item for item in classified if (2 not in [item['pol_textblob'], item['pol_nlpWarriers']])\n",
    "                           and (item['pol_textblob'] != item['pol_nlpWarriers'])])\n",
    "    output = output[['video_id', 'comment', 'pol_textblob', 'pol_nlpWarriers']]\n",
    "    output.to_csv('output/nn_disagree.csv')\n",
    "    \n",
    "    output = pd.DataFrame([item for item in classified if (item['pol_textblob'] == item['pol_nlpWarriers'])])\n",
    "    output = output[['video_id', 'comment', 'pol_textblob', 'pol_nlpWarriers']]\n",
    "    output.to_csv('output/agree.csv')\n",
    "    \n",
    "    output = pd.DataFrame([item for item in classified if (item['pol_textblob'] != item['pol_nlpWarriers'])])\n",
    "    output = output[['video_id', 'comment', 'pol_textblob', 'pol_nlpWarriers']]\n",
    "    output.to_csv('output/disagree.csv')\n",
    "\n",
    "\n",
    "\n",
    "disagree = 0\n",
    "disagree_no_neutral = 0\n",
    "no_neutral = 0\n",
    "for i, row in df_usa_comments.iterrows():\n",
    "    # cont_from = 70000\n",
    "    # if (i < cont_from):\n",
    "    #     continue\n",
    "    # elif (i == cont_from):\n",
    "    #     print(\"continue from {}\".format(i))\n",
    "\n",
    "    if (i == 20):\n",
    "        save_progress(classified)\n",
    "\n",
    "    tb = TextBlob(text=str(row['tweet']))\n",
    "    pol_tb = tb.polarity\n",
    "    if (pol_tb < 0):\n",
    "        pol_tb = 'negative'\n",
    "    elif (pol_tb > 0):\n",
    "        pol_tb = 'positive'\n",
    "    else:\n",
    "        pol_tb = 'neutral'\n",
    "\n",
    "    tweet_data = pd.DataFrame({'tweet': [str(row['tweet'])]})\n",
    "    pol_clf = clf.classify(contains_feature(preprocess_all(tweet_data).at[0, 'tweet']))\n",
    "\n",
    "    classified.append({'video_id': row['video_id'],\n",
    "                       'comment': row['tweet'],\n",
    "                       'pol_textblob': reverrse_polarity_mapping(pol_tb),\n",
    "                       'pol_nlpWarriers': reverrse_polarity_mapping(pol_clf)\n",
    "                       })\n",
    "\n",
    "    if ('neutral' not in [pol_tb, pol_clf]):\n",
    "        no_neutral += 1\n",
    "\n",
    "    if (pol_tb != pol_clf):\n",
    "        disagree += 1\n",
    "        if ('neutral' not in [pol_tb, pol_clf]):\n",
    "            disagree_no_neutral += 1\n",
    "\n",
    "    # if ((i % 500) == 0):\n",
    "    #     print(row['tweet'], pol_clf, pol_tb)\n",
    "\n",
    "    if ((i % 10000) == 0):\n",
    "        print(i, disagree, no_neutral, disagree_no_neutral, (disagree_no_neutral / max(no_neutral, 1)) * 100)\n",
    "        save_progress(classified)\n",
    "\n",
    "print(disagree, (disagree / len(df_usa_comments)) * 100, disagree_no_neutral, (disagree_no_neutral / no_neutral) * 100)\n",
    "\n",
    "save_progress(classified)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing our labelled versions of the output datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233076"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(\"./output/agree.csv\",error_bad_lines=False)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116538\n"
     ]
    }
   ],
   "source": [
    "df = df[len(df)//2:]\n",
    "print(len(df))\n",
    "df_shuffled = df.sample(frac=1)\n",
    "df_shuffled.to_csv(\"output/agree_shuffled_dennis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textblob|Our_classifier comparison\nTestset size: 197 without neutral: 122\nTextblob Accuracy: 25.41\nOur Classifier Accuracy: 59.02\n"
     ]
    }
   ],
   "source": [
    "test_set_youtube = pd.read_csv(\"./output/disagree_shuffled_dennis.csv\",error_bad_lines=False)\n",
    "test_set_youtube_2 = pd.read_csv(\"./output/disagree_shuffled_joe.csv\",error_bad_lines=False)\n",
    "test_set_youtube = pd.concat([test_set_youtube,test_set_youtube_2])\n",
    "test_set_youtube.drop(test_set_youtube.columns[[0,1]], axis= 1, inplace=True)\n",
    "\n",
    "def percentage(num,of):\n",
    "    return (num/of)*100\n",
    "\n",
    "no_neutral = 0\n",
    "correct_clf = 0\n",
    "correct_tb = 0\n",
    "for i, row in test_set_youtube.iterrows():\n",
    "    if row['pol_human'] != 2:\n",
    "        no_neutral += 1\n",
    "        if row['pol_textblob'] == row['pol_human']:\n",
    "            correct_tb += 1\n",
    "        elif row['pol_nlpWarriers'] == row['pol_human']:\n",
    "            correct_clf += 1\n",
    "            \n",
    "print(\"Textblob|Our_classifier comparison\\nTestset size: {} without neutral: {}\\n\"\n",
    "      \"Textblob Accuracy: {:.02f}\\nOur Classifier Accuracy: {:.02f}\".format(\n",
    "    len(test_set_youtube),no_neutral,percentage(correct_tb,no_neutral),percentage(correct_clf,no_neutral)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textblob|Our_classifier accuracy when in agreement\nTestset size: 199 without neutral: 169\nAccuracy: 90.53\n"
     ]
    }
   ],
   "source": [
    "test_set_youtube = pd.read_csv(\"./output/agree_shuffled_dennis.csv\",error_bad_lines=False)\n",
    "test_set_youtube_2 = pd.read_csv(\"./output/agree_shuffled_joe.csv\",error_bad_lines=False)\n",
    "test_set_youtube = pd.concat([test_set_youtube,test_set_youtube_2])\n",
    "test_set_youtube.drop(test_set_youtube.columns[[0,1]], axis= 1, inplace=True)\n",
    "\n",
    "def percentage(num,of):\n",
    "    return (num/of)*100\n",
    "\n",
    "no_neutral = 0\n",
    "correct = 0\n",
    "for i, row in test_set_youtube.iterrows():\n",
    "    if row['pol_human'] != 2:\n",
    "        no_neutral += 1\n",
    "        if row['pol_textblob'] == row['pol_human']:\n",
    "            correct += 1\n",
    "    \n",
    "            \n",
    "print(\"Textblob|Our_classifier accuracy when in agreement\\nTestset size: {} without neutral: {}\\n\"\n",
    "      \"Accuracy: {:.02f}\".format(\n",
    "    len(test_set_youtube),no_neutral,percentage(correct,no_neutral)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
